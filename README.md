# CodeDeck

A slide deck for code. Define frames, step through them, export as video.

CodeDeck turns an array of editor states into a presentation you can step through with arrow keys. Each frame describes what the file tree looks like, which file is open, what the code says, where to scroll, and what to highlight — plus optional narration for TTS audio. That's it.

No timeline editors. No drag and drop. No render times. Just declare your states and present.

## Quick start

```
.
├── index.html              # the viewer
├── frames.md               # your presentation source (author here)
├── frames.json             # generated intermediate data
├── frames.js               # generated presentation data (loaded by index.html)
├── parse-frames.js         # Node.js parser: frames.md → frames.json
├── generate_voices.py      # TTS audio generator (pocket-tts)
├── build-images.sh         # export frames as PNG screenshots (Firefox headless)
├── make_video.sh           # stitch PNGs + audio into MP4 (FFmpeg)
└── build.sh                # run the full pipeline
```

Open `index.html` in a browser. Use **Left** / **Right** arrow keys to step through frames. Append `?frame=N` to the URL to jump to a specific slide.

## Authoring frames

Edit `frames.md`. Frames are separated by `---`. Each frame has metadata lines followed by a fenced code block:

~~~
text: Narration text read aloud during this frame.
selectedFile: src/index.js
scrollLine: 5
highlights: [3, 4, 5]

```javascript
console.log('hello world');
```
~~~

The `text` and `selectedFile` fields are required. The parser builds the file tree automatically from all `selectedFile` paths seen across all frames.

## Build pipeline

Run the full pipeline at once:

```sh
bash build.sh
```

Or run steps individually:

| Step | Command | Output |
|------|---------|--------|
| Parse frames | `node parse-frames.js frames.md > frames.json` | `frames.json` |
| Generate JS | `bash generate_frames_js.sh` | `frames.js` |
| View in browser | open `index.html` | — |
| Export images | `bash build-images.sh` | `images/frame-NNN.png` |
| Generate audio | `uv run python generate_voices.py` | `audio/frame-NNN.wav` |
| Create video | `bash make_video.sh` | `output.mp4` |

## Frame schema

| Field | Type | Description |
|-------|------|-------------|
| `text` | `string` | Narration text used for TTS audio generation. |
| `selectedFile` | `string` | Path of the currently open file. |
| `tree` | `{ path, type }[]` | File tree entries. Auto-generated by the parser. |
| `language` | `string` | Language identifier for highlight.js (e.g. `"javascript"`, `"json"`). |
| `content` | `string` | Plain text content of the open file. |
| `scrollLine` | `number` | 1-indexed line to scroll into view. `0` resets scroll to top. |
| `highlights` | `number[]` | 1-indexed line numbers to highlight. |

## Exporting frames as images

`build-images.sh` uses Firefox in headless mode to capture each frame as a PNG and saves them to `images/`.

```sh
bash build-images.sh
```

> **Note:** Firefox must be fully closed before running this script. If a Firefox instance is already open, headless mode will try to reuse the existing session and screenshots will not be taken.

## Generating narration audio

`generate_voices.py` reads the `text` field from each frame in `frames.json` and generates a WAV file per frame using the [pocket-tts](https://github.com/marytts/pocket-tts) library.

**Dependencies** (managed via `pyproject.toml`):

```sh
uv sync
```

**Usage:**

```sh
uv run python generate_voices.py
```

Output WAV files are saved to `audio/frame-NNN.wav`.

## Stitching frames into a video

`make_video.sh` stitches the exported PNGs and per-frame audio into a video with crossfade transitions using FFmpeg.

**Dependency:**

```sh
sudo apt install ffmpeg
```

**Usage:**

```sh
bash make_video.sh
```

Output is saved to `output.mp4`. Each frame's duration is determined by its audio length. Frames without audio fall back to a configurable default duration. Edit the variables at the top of `make_video.sh` to change fade length, resolution, FPS, or output path.

## Syntax highlighting

CodeDeck uses [highlight.js](https://highlightjs.org/) loaded from CDN. JavaScript is included by default. To add more languages, add the corresponding `<script>` tag from [cdnjs](https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/languages/):

```html
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/languages/python.min.js"></script>
```

If highlight.js doesn't have the requested language registered, it falls back to plain unstyled text.

## Adding Background Music (YouTube Mix)

After generating `output.mp4`, you can add background music suitable for YouTube content.

### Getting Music

You can download royalty-free music from the **YouTube Audio Library** inside **YouTube Studio**:

1. Go to YouTube Studio
2. Navigate to **Audio Library**
3. Download a suitable background track (ambient / minimal recommended)

Avoid tracks with strong vocals or aggressive percussion, as they can compete with narration.

## Mixing Voice + Music with FFmpeg

The following command:

* Keeps narration clear and upfront
* EQs the music to avoid competing with speech
* Automatically ducks music when voice is present
* Compresses narration for consistency
* Outputs YouTube-friendly AAC audio

```bash
ffmpeg -i output.mp4 -i "On The Flip - The Grey Room _ Density & Time.mp3" \
-filter_complex "\
[1:a]volume=0.18,highpass=f=120,lowpass=f=8000, \
equalizer=f=2000:t=q:w=1:g=-6[a_music]; \
[a_music][0:a]sidechaincompress=threshold=0.02:ratio=10:attack=15:release=400[ducked]; \
[0:a]acompressor=threshold=-18dB:ratio=3:attack=5:release=100[voice]; \
[voice][ducked]amix=inputs=2:weights=2 1:duration=first:dropout_transition=2[aout]" \
-map 0:v -map "[aout]" \
-c:v copy -c:a aac -b:a 192k -shortest final.mp4
```

### Output

Produces:

```
final.mp4
```

This version:

* Prioritizes narration clarity
* Keeps music subtle and non-distracting
* Automatically matches video duration
* Is optimized for YouTube loudness normalization

## License

MIT
